extends ../_layout

block vars
  - p = projectsTable['picode'];
  - title = p.getTitle(lang);
  - me = p.project + '/';

block main
  +render-project-header('picode')
  .ui.message
    block message
      | The project was implemented using the 
      a(href=`${basePath}phybots`) Phybots
      |  library for creating robotic things and its source code is available on 
      a(href='https://github.com/arcatdmz/picode/') GitHub
      | ! The development was supported in part by 
      a(href='https://www.microsoft.com/ja-jp/ijarc/core/ifp_07_j_result.aspx') Microsoft Research CORE 7 program (Japanese page)
      | .
  .ui.hidden.divider
  section#abstract
    h2.ui.header
      .content #{en ? 'Abstract' : '概要'}
    .ui.basic.segment
      block abstract
        p Current programming environments use textual or symbolic representations. While these representations are good at describing logical processes, they are not appropriate for representing human and robot posture data which is necessary for handing gesture input and controlling robots.
        p To address this issue, we propose a text-based development environment integrated with visual representations: photos of human and robots. With our development environment, the user first takes a photo to bind it to a posture data. Then, s/he drag-and-drops the photo to the code editor, where it is shown as an inline image.
  .ui.hidden.divider
  section#presentations
    h2.ui.header #{en ? 'Presentation materials' : '発表内容'}
    .ui.basic.segment
      .ui.stackable.two.columns.grid
        .column
          h3.ui.header CHI 2013 Fast-forward
          +embed-youtube('//www.youtube.com/embed/NCou4Q1STE8')
          .caption: .ui.horizontal.list
            .item
              a(href='//www.youtube.com/watch?v=NCou4Q1STE8')
                i.youtube.icon
                | Fast-forward
            .item
              a(href='//www.youtube.com/watch?v=cQv0-4f8LEA')
                i.youtube.icon
                | Full version
        .column
          h3.ui.header CHI 2013 #{en ? 'Slides' : '発表スライド'}
          .ui.embed(data-url=publicationsTable[p.publication].entryTags.slides, data-placeholder=`${rootPath}images/background-eee.png`, data-icon='video play outline')
          .caption: .ui.horizontal.list
            .item
              a(href=publicationsTable[p.publication].entryTags.slides)
                i.download.icon
                | #{en ? 'Download' : 'ダウンロード'}
          - if (!en) {
          p.caption 日本語の発表スライドは
            a(href='#publications')
              i.book.icon
              | 発表文献
            | で日本語文献の 
            i.sidebar.fitted.icon
            |  をクリックすると閲覧できます。
          - }
  .ui.hidden.divider
  section
    h2.ui.header Picode IDE
    .ui.basic.segment
      block overview
        p Our prototype implementation consists of three main components: the code editor, the pose library, and the preview window. First, the user takes a photo of a human or robot in the preview window. At the same time, posture data is captured and the dataset is stored in the pose library. Next, s/he drag-and-drops the photo from the pose library into the code editor, where the photo is shown inline. Then, s/he can run the application. S/he can also distribute the source code bundled with the referenced datasets so that others can run the same application on our development environment.
        p
          | During the coding phase, the programmer can benefit from a software library tightly coupled with the IDE. The library provides a set of API that enables easy control of robots. The main functions are shown below. Other APIs are listed in 
          a(href=`${rootPath}picode/docs/?com/phybots/picode/api/package-summary.html`) Javadoc
          | .
    .ui.stackable.two.columns.grid
      .column
        a.ui.fluid.image(href=`${rootPath}picode/picode-ide.jpg`)
          img(src=`${rootPath}picode/picode-ide.jpg`, alt='Overview')
        .caption #{en ? 'Overview of the Picode IDE' : 'Picode IDEの概観'}
      .column
        block api
          dl
            dt <strong>Pose</strong> Robot.getPose()
            dd returns the current pose data.
            dt <strong>float</strong> Pose.distance(Pose pose)
            dd calculates the distance between the two poses. [0.0-1.0]
            dt <strong>boolean</strong> Pose.eq(Pose pose, float threshold)
            dd returns whether the two poses can be thought of as identical or not. (returns whether the distance between the two poses is less than the threshold or not.)
            dt <strong>boolean</strong> Robot.setPose(Pose pose)
            dd set the current pose to the specified data.
            dt <strong>Action</strong> Robot.action()
            dd start building an action for the robot.
            dt <strong>Action</strong> Action.pose(Pose pose)
            dd add this pose to the end of this action.
            dt <strong>Action</strong> Action.stay(int ms)
            dd wait for the specified time at the end of this action.
            dt <strong>ActionResult</strong> Action.play()
            dd play this action.
  - if (!en) {
  .ui.hidden.divider
  section#vision
    h2.ui.header 未来ビジョン
    .ui.basic.segment
      block extra
  - }
  .ui.hidden.divider
  +render-project-publications('picode', 'chi2013-kato-picode')